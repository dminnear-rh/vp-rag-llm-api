# ğŸ” RAG API for Validated Patterns

This project provides a FastAPI application that serves as a Retrieval-Augmented Generation (RAG) backend, optimized for answering questions about Validated Patterns using a hybrid of vector search and LLMs.

## âœ¨ Features

- ğŸ”— **Retrieval-Augmented Generation** with Qdrant vector search
- ğŸ§  **LLM Integration** with [vLLM](https://github.com/vllm-project/vllm) or [OpenAI Chat Completions API](https://platform.openai.com/docs/guides/gpt)
- ğŸ“š **Semantic re-ranking** using a cross-encoder (MiniLM)
- ğŸ’¬ **Conversation history support**
- ğŸ“¡ **Streaming & non-streaming completions**
- âš™ï¸ **Dynamic multi-model support** via `.env` or environment variables
- ğŸ“¦ **Lightweight container image** (based on UBI9 Python 3.12)

## ğŸ§ª Example Usage

```bash
curl -X POST http://localhost:8080/rag-query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "How are secrets managed in validated patterns?",
    "history": []
  }'
```

Streaming:

```bash
curl -N -X POST http://localhost:8080/rag-query/stream \
  -H "Content-Type: application/json" \
  -d '{"question": "What does the travelops pattern do?"}'
```

## ğŸ§° Configuration

All settings are handled via a `.env` file â€” and any value can also be overridden via standard environment variables:

```env
# === Qdrant Vector Store ===
QDRANT_URL=http://qdrant:6333
QDRANT_COLLECTION=rag-collection

# === Default Model Name ===
DEFAULT_MODEL=mistral

# === Model Registry (JSON array)
# model_type: one of "vllm" | "openai" | "anthropic" | etc.
# max_tokens is the total usable for prompt + response.
VLLM_MODELS=[
  {
    "name": "mistral",
    "url": "http://vllm-mistral:8000",
    "api_key": "",
    "model_type": "vllm",
    "max_total_tokens": 32768
  },
  {
    "name": "llama4",
    "url": "http://vllm-llama4:8000",
    "api_key": "",
    "model_type": "vllm",
    "max_total_tokens": 32768
  },
  {
    "name": "gpt-4",
    "url": "https://api.openai.com/v1/chat/completions",
    "api_key": "sk-...",
    "model_type": "openai",
    "max_total_tokens": 8192
  }
]

# === Logging ===
LOG_LEVEL=info
```

> ğŸ“ See `config.py` for full validation of model and environment values.

## ğŸ³ Running Locally

Build the container:

```bash
podman build -t rag-api .
```

Run it:

```bash
podman run -p 8080:8080 --env-file .env rag-api
```

## ğŸ“ Project Structure

```
â”œâ”€â”€ main.py          # App entrypoint
â”œâ”€â”€ router.py        # Routes for /rag-query and /rag-query/stream
â”œâ”€â”€ config.py        # AppConfig loading from .env
â”œâ”€â”€ llm.py           # LLM routing for OpenAI/vLLM
â”œâ”€â”€ retrieval.py     # Embedding + Qdrant search + cross-encoder rerank
â”œâ”€â”€ models.py        # Model dataclasses + enums
â”œâ”€â”€ utils.py         # Token counting, trimming, helpers
â””â”€â”€ .env             # Local development configuration
```
